{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf34b38b",
   "metadata": {},
   "source": [
    "## LODING THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe50fc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#DataFlair Project\n",
    "#import all the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from statistics import mode\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78443c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f2bc0",
   "metadata": {},
   "source": [
    "## FETCHING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e58ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#read the dataset file\n",
    "df=pd.read_csv(\"reviews_df1.csv\")\n",
    "#drop the duplicate and na values from the records\n",
    "df.drop_duplicates(subset=['Text'],inplace=True)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "input_data = df.loc[:,'Text']\n",
    "target_data = df.loc[:,'Summary']\n",
    "#target.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f99efa",
   "metadata": {},
   "source": [
    "## TEXT PREPOSESSIING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26fdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_words=[]\n",
    "target_words=[]\n",
    "contractions= pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n",
    "#initialize stop words and LancasterStemmer\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words.remove(\"not\")\n",
    "stemm=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7501f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(texts,src):\n",
    "  #remove the html tags\n",
    "  texts = BeautifulSoup(texts, \"lxml\").text\n",
    "  #tokenize the text into words \n",
    "  words=word_tokenize(texts.lower())\n",
    "  #filter words which contains \\ \n",
    "  #integers or their length is less than or equal to 3\n",
    "  words= list(filter(lambda w:(w.isalpha() and len(w)>=3),words))\n",
    "  #contraction file to expand shortened words\n",
    "  words= [contractions[w] if w in contractions else w for w in words ]\n",
    "  #stem the words to their root word and filter stop words\n",
    "  if src==\"inputs\":\n",
    "    words= [stemm.stem(w) for w in words if w not in stop_words]\n",
    "  else:\n",
    "    words= [w for w in words if w not in stop_words]\n",
    "  return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d34331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input words :  33906\n",
      "number of target words :  14870\n",
      "maximum input length :  87\n",
      "maximum target length :  19\n"
     ]
    }
   ],
   "source": [
    "#pass the input records and taret records\n",
    "for in_txt,tr_txt in zip(input_data,target_data):\n",
    "  in_words= clean(in_txt,\"inputs\")\n",
    "  input_texts+= [' '.join(in_words)]\n",
    "  input_words+= in_words\n",
    "  #add 'sos' at start and 'eos' at end of text\n",
    "  tr_words= clean(\"sos \"+tr_txt+\" eos\",\"target\")\n",
    "  target_texts+= [' '.join(tr_words)]\n",
    "  target_words+= tr_words\n",
    "\n",
    "#store only unique words from input and target list of words\n",
    "input_words = sorted(list(set(input_words)))\n",
    "target_words = sorted(list(set(target_words)))\n",
    "num_in_words = len(input_words) #total number of input words\n",
    "num_tr_words = len(target_words) #total number of target words\n",
    "\n",
    "#get the length of the input and target texts which appears most often  \n",
    "max_in_len = mode([len(i) for i in input_texts])\n",
    "max_tr_len = mode([len(i) for i in target_texts])\n",
    "\n",
    "print(\"number of input words : \",num_in_words)\n",
    "print(\"number of target words : \",num_tr_words)\n",
    "print(\"maximum input length : \",max_in_len)\n",
    "print(\"maximum target length : \",max_tr_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a5f30",
   "metadata": {},
   "source": [
    "## SPLITING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3afd0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the input and target text into 80:20 ratio or testing size of 20%.\n",
    "x_train,x_test,y_train,y_test=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0)\n",
    "\n",
    "#train the tokenizer with all the words\n",
    "in_tokenizer = Tokenizer()\n",
    "in_tokenizer.fit_on_texts(x_train)\n",
    "tr_tokenizer = Tokenizer()\n",
    "tr_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "#convert text into sequence of integers\n",
    "#where the integer will be the index of that word\n",
    "x_train= in_tokenizer.texts_to_sequences(x_train) \n",
    "y_train= tr_tokenizer.texts_to_sequences(y_train) \n",
    "\n",
    "#pad array of 0's if the length is less than the maximum length \n",
    "en_in_data= pad_sequences(x_train,  maxlen=max_in_len, padding='post') \n",
    "dec_data= pad_sequences(y_train,  maxlen=max_tr_len, padding='post')\n",
    "\n",
    "#decoder input data will not include the last word \n",
    "#i.e. 'eos' in decoder input data\n",
    "dec_in_data = dec_data[:,:-1]\n",
    "#decoder target data will be one time step ahead as it will not include\n",
    "# the first word i.e 'sos'\n",
    "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]\n",
    "\n",
    "K.clear_session() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4afc1",
   "metadata": {},
   "source": [
    "## PRIDICTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9be817a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter : Poor quality service.We had to wait a good 30 minutes before someone noticed us and the restaurant was practically empty at that Time . The food was mediocre too.Never recommending this to anybody.\n",
      "Review : Poor quality service.We had to wait a good 30 minutes before someone noticed us and the restaurant was practically empty at that Time . The food was mediocre too.Never recommending this to anybody.\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "\n",
      "Predicted summary: not good  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in_tokenizer = joblib.load(\"in_tokenizer4.pkl\")\n",
    "# tr_tokenizer = joblib.load(\"in_tokenizer4.pkl\")\n",
    "\n",
    "from tensorflow import keras\n",
    "# encoder inference\n",
    "latent_dim=500\n",
    "#load the model\n",
    "model = keras.models.load_model('C:\\\\Users\\\\rohitha\\\\Downloads\\\\text-summarization-ml-project\\\\new4.h5')\n",
    "max_in_len=87\n",
    "#construct encoder model from the output of 6 layer i.e.last LSTM layer\n",
    "en_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
    "en_states=[state_h_enc,state_c_enc]\n",
    "#add input and state from the layer.\n",
    "en_model = Model(model.input[0],[en_outputs]+en_states)\n",
    "\n",
    "# decoder inference\n",
    "#create Input object for hidden and cell state for decoder\n",
    "#shape of layer with hidden or latent dimension\n",
    "dec_state_input_h = Input(shape=(latent_dim,))\n",
    "dec_state_input_c = Input(shape=(latent_dim,))\n",
    "dec_hidden_state_input = Input(shape=(max_in_len,latent_dim))\n",
    "\n",
    "# Get the embeddings and input layer from the model\n",
    "dec_inputs = model.input[1]\n",
    "dec_emb_layer = model.layers[5]\n",
    "dec_lstm = model.layers[7]\n",
    "dec_embedding= dec_emb_layer(dec_inputs)\n",
    "\n",
    "#add input and initialize LSTM layer with encoder LSTM states.\n",
    "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])\n",
    "\n",
    "#Attention layer\n",
    "attention = model.layers[8]\n",
    "attn_out2 = attention([dec_outputs2,dec_hidden_state_input])\n",
    "\n",
    "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out2])\n",
    "\n",
    "#Dense layer\n",
    "dec_dense = model.layers[10]\n",
    "dec_outputs2 = dec_dense(merge2)\n",
    "\n",
    "# Finally define the Model Class\n",
    "dec_model = Model(\n",
    "[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],\n",
    "[dec_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "#create a dictionary with a key as index and value as words.\n",
    "reverse_target_word_index = tr_tokenizer.index_word\n",
    "reverse_source_word_index = in_tokenizer.index_word\n",
    "target_word_index = tr_tokenizer.word_index\n",
    "reverse_target_word_index[0]=' '\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    #get the encoder output and states by passing the input sequence\n",
    "    en_out, en_h, en_c= en_model.predict(input_seq)\n",
    "\n",
    "    #target sequence with inital word as 'sos'\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_word_index['sos']\n",
    "\n",
    "    #if the iteration reaches the end of text than it will be stop the iteration\n",
    "    stop_condition = False\n",
    "    #append every predicted word in decoded sentence\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: \n",
    "        #get predicted output, hidden and cell state.\n",
    "        output_words, dec_h, dec_c= dec_model.predict([target_seq] + [en_out,en_h, en_c])\n",
    "        \n",
    "        #get the index and from the dictionary get the word for that index.\n",
    "        word_index = np.argmax(output_words[0, -1, :])\n",
    "        text_word = reverse_target_word_index[word_index]\n",
    "        decoded_sentence += text_word +\" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find a stop word or last word.\n",
    "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
    "          stop_condition = True\n",
    "        \n",
    "        #update target sequence to the current word index.\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = word_index\n",
    "        en_h, en_c = dec_h, dec_c\n",
    "    \n",
    "    #return the deocded sentence\n",
    "    return decoded_sentence\n",
    "\n",
    "inp_review = input(\"Enter : \")\n",
    "print(\"Review :\",inp_review)\n",
    "\n",
    "inp_review = clean(inp_review,\"inputs\")\n",
    "inp_review = ' '.join(inp_review)\n",
    "inp_x= in_tokenizer.texts_to_sequences([inp_review]) \n",
    "inp_x= pad_sequences(inp_x,  maxlen=max_in_len, padding='post')\n",
    "\n",
    "summary=decode_sequence(inp_x.reshape(1,max_in_len))\n",
    "if 'eos' in summary :\n",
    "  summary=summary.replace('eos','')\n",
    "print(\"\\nPredicted summary:\",summary);print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8db56",
   "metadata": {},
   "source": [
    "### Now we want to check whethere the pridicted summary is POSITIVE or NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "253fc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "vec = joblib.load(\"vec.pkl\")\n",
    "classifier_rf = joblib.load(\"textsummarization1.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1a2fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words(\"english\")\n",
    "stopword.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1681b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "#     raw_text = str(raw_text)\n",
    "    sentence = re.sub(\"[^a-zA-Z0-9]\", \" \", raw_text)\n",
    "#     sentence = re.sub(r'[0-9]', '',str(raw_text))\n",
    "#     sentence = sentence.isalpha()\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    tokens = sentence.split()          \n",
    "    clean_tokens = [t for t in tokens if not t in stopwords.words(\"english\")]\n",
    "    clean_tokens = [lemma.lemmatize(word) for word in clean_tokens]\n",
    "    \n",
    "    return pd.Series([\" \".join(clean_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b448e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "text = summary\n",
    "x = preprocess(text)\n",
    "v1 = vec.transform(x)\n",
    "p = classifier_rf.predict(v1)\n",
    "print(p[0])\n",
    "if p[0]==\"-1\":\n",
    "    print(\"Negative\")\n",
    "elif p[0]==\"1\":\n",
    "    print(\"positive\")\n",
    "else:\n",
    "    print(\"neutral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1fd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794e3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b324823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f210d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
